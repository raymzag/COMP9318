{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP-9318 Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions:\n",
    "1. This note book contains instructions for **COMP9318 Final-Project**.\n",
    "\n",
    "* You are required to complete your implementation in a file `submission.py` provided along with this notebook.\n",
    "\n",
    "* You are not allowed to print out unnecessary stuff. We will not consider any output printed out on the screen. All results should be returned in appropriate data structures returned by corresponding functions.\n",
    "\n",
    "* This notebook encompasses all the requisite details regarding the project. Detailed instructions including **CONSTRAINTS**, **FEEDBACK** and **EVALUATION** are provided in respective sections. In case of additional problem, you can post your query @ Piazza.\n",
    "\n",
    "* This project is **time-consuming**, so it is highly advised that you start working on this as early as possible.\n",
    "\n",
    "* You are allowed to use only the permitted libraries and modules (as mentioned in the **CONSTRAINTS** section). You should not import unnecessary modules/libraries, failing to import such modules at test time will lead to errors.\n",
    "\n",
    "* You are **NOT ALLOWED** to use dictionaries and/or external data resources for this project.\n",
    "\n",
    "* We will provide you **LIMITED FEEDBACK** for your submission (only **15** attempts allowed to each group). Instructions for the **FEEDBACK** and final submission are given in the **SUBMISSION** section.\n",
    "\n",
    "* For **Final Evaluation** we will be using a different dataset, so your final scores may vary.  \n",
    "\n",
    "* Submission deadline for this assignment is **23:59:59 on 27-May, 2018**.\n",
    "* **Late Penalty: 10-% on day-1 and 20% on each subsequent day.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction:\n",
    "\n",
    "In this Project, you are required to devise an algorithm/technique to fool a binary classifier named `target-classifier`. In this regard, you only have access to following information:\n",
    "\n",
    "<br>\n",
    "1. The `target-classifier` is a binary classifier classifying data to two categories, $\\textit{i.e.}$, **class-1** and **class-0**.\n",
    "\n",
    "2. You have access to part of classifiers' training data, $\\textit{i.e.}$, a sample of 540 paragraphs. 180 for **class-1**, and 360 for **class-0**, provided in the files: `class-1.txt` and `class-0.txt` respectively.\n",
    "\n",
    "3. The `target-classifier` belong to the SVM family.\n",
    "\n",
    "4. The `target-classifier` allows **EXACTLY 20 DISTINCT** modifications in each test sample.\n",
    "5. You are provided with a test sample of **200** paragraphs from **class-1** (in the file: `test_data.txt`). You can use these test samples to get feedback from the target classifier (**only 15 attempts** allowed to each group.).\n",
    "6. **NOTE: You are not allowed to use the data `test_data.txt` for your model training (if any). VIOLATIONS in this regard will get ZERO score**.\n",
    "\n",
    "<br>\n",
    "### -to-do:\n",
    "* You are required to come up with an algorithm named `fool_classifier()` that makes best use of the above-mentioned information (**point 1-4**) to fool the `target-classifier`. By fooling the classifier we mean that your algorithm can help mis-classify a bunch of test instances (**point-5**) with minimal possible modifications (**EXACTLY 20 DISTINCT** modifications allowed to each test sample). \n",
    "\n",
    "* **NOTE::** We put a **harsh limit** on the number of modifications allowed for each test instance. You are only allowed to modify each test sample by **EXACTLY 20 DISTINCT tokens (NO MORE NO LESS)**.\n",
    "\n",
    "* **NOTE::** **ADDING** or **DELETING** one word at a time is **ONE** modification. Replacement will be considered as **TWO** modifications $(\\textit{i.e.,}$ **Deletion** followed by **Insertion**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraints\n",
    "\n",
    "Your implementation `submission.py` should comply with following constraints.\n",
    "\n",
    "1. You should implement your methodology using `Python3`.\n",
    "* You should implement your code in the function `fool_classifier()` in the file `submission.py`. \n",
    "* You are only allowed to use pre-defined class `strategy()` defined in the file: `helper.py` in order to train your models (if any). \n",
    "* You **should not** do any pre-processing on the data. We have already pre-processed the data for you.  \n",
    "* You are supposed to implement your algorithm using **scikit-learn (version=0.19.1)**. We will **NOT** accept implementations using other Libraries.\n",
    "\n",
    "* You are **not supposed to augment** the data using external/additional resources.  You are only allowed to use the partial training data provided to you ($\\textit{i.e.,} $ `class-1.txt` and `class-0.txt`).\n",
    "\n",
    "* You are **not** allowed to use the test samples ($\\textit{i.e.,}$ `test_data.txt`) for model training and/or inference building. You can only use this data for testing, $\\textit{i.e.,}$ calculating success %-age (as described in the **EVALUATION** section.). **VIOLATIONS IN THIS REGARD WILL GET ZERO SCORE**.\n",
    "\n",
    "* You are **not** allowed to hard code the ground truth and any other information into your implementation `submission.py`. \n",
    "\n",
    "* Considering the **RUNNING TIME**, your implementation is supposed to read the test data file ($\\textit{i.e.,}$ `test_data.txt` with 200 test samples), process it and write the modified file (`modified_data.txt`) within **12 Minutes**.\n",
    "\n",
    "* Each modified test sample in the modified file (`modified_data.txt`) should not differ from the original test sample corresponding to the file (`test_data.txt`) by more than 20 tokens.\n",
    "\n",
    "* **NOTE::** Inserting or Deleting a word is **ONE** modification. Replacement will be considered as **TWO** modifications $(\\textit{i.e.,}$ deletion followed by insertion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions:\n",
    "\n",
    "* Please read these instructions **VERY CAREFULLY**.\n",
    "\n",
    "### FEEDBACK:\n",
    "* For this project, we will provide real-time feed-back on a test data ($\\textit{i.e.,}$ the file `test_data.txt` containing **200** test cases).\n",
    "* Each group is allowed to avail only **15 attempts in TOTAL**, so use your attempts **WISELY**.\n",
    "* We will only provide **ACCUMULATIVE FEEDBACK** ($\\textit{i.e.,}$ how many modified test samples out of **200** were classified as Class-0). We **WILL NOT** provide detailed feedback for individual test cases.\n",
    "* For the feedback, you are required to submit the modified text file ($\\textit{i.e.,}$ `modified_data.txt`) via the submission portal: http://kg.cse.unsw.edu.au:8318/project/ (using Group name and Group password).\n",
    "* **NOTE::** Please make sure that the modified text file is generated by your program `fool_classifier()`, and it obeys the modification constraints. We have provided a function named: `check_data()` in the class: `strategy()`to check whether the modified file: `modified_data.txt` obeys the constraints.\n",
    "\n",
    "3. Your algorithm should modify each test sample in `test_data.txt` by **EXACTLY 20 DISTINCT TOKENS**.\n",
    "\n",
    "### Final Submission:\n",
    "1. For final submission, you need to submit:\n",
    "    * Your code in the file `submission.py`\n",
    "    * A report (`report.pdf`) outlining your approach for this project.\n",
    "2. We will release the detailed instructions for the final submission submission via Piazza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details\n",
    "\n",
    "1. In the file `submission.py`, you are required to implement a function named: `fool_classifier()` that reads a text file named: `test_data.txt` from Present Working Directory(PWD), and writes out the modified text file: `modified_data.txt` in the same directory.\n",
    "* We have provided the implementation of **strategy** class in a seperate file `helper.py`. You are supposed to use this class for your model training (if any) and inference building.\n",
    "\n",
    "* **Detailed description of input and/or output parts is given below:**\n",
    "\n",
    "### Input: \n",
    "* The function `fool_classifier()` reads a text files named `test_data.txt` having almost (500-1500) test samples. Each line in the input file corresponds to a single test sample.\n",
    "\n",
    "* **Note:** We will also provide the partial training data ($\\textit{(i)}$ `class-0.txt` and $\\textit{(ii)}$ `class-1.txt`) in the test environment. You can  access this data using the class: `strategy()`. \n",
    "\n",
    "### Output:\n",
    "* You are supposed to write down the modified file named `modified_data.txt` in the same directory, and in the same format as that of the `test_data.txt`. In addition, your program is supposed to return the instance of the `strategy` class defined in `helper.py`.\n",
    "\n",
    "\n",
    "* **Note:** Please make sure that the file: `modified_data.txt` is generated by your code, and it follows the **MODIFICATION RESTRICTIONS (ADD** and/or **DELETE EXACTLY 20 DISTINCT TOKENS)**. In case of **ERRORS**, we will **NOT** allow more feedback attempts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# We have provided these implementations in the file helper.py, provided along with this project.\n",
    "## Please do not change these functions.\n",
    "###################\n",
    "class countcalls(object):\n",
    "    __instances = {}\n",
    "    def __init__(self, f):\n",
    "        self.__f = f\n",
    "        self.__numcalls = 0\n",
    "        countcalls.__instances[f] = self\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        self.__numcalls += 1\n",
    "        return self.__f(*args, **kwargs)\n",
    "    @staticmethod\n",
    "    def count(f):\n",
    "        return countcalls.__instances[f].__numcalls\n",
    "    @staticmethod\n",
    "    def counts():\n",
    "        res = sum(countcalls.count(f) for f in countcalls.__instances)\n",
    "        for f in countcalls.__instances:\n",
    "            countcalls.__instances[f].__numcalls = 0\n",
    "        return res\n",
    "    \n",
    "## Strategy() class provided in helper.py to facilitate the implementation.\n",
    "class strategy:\n",
    "    ## Read in the required training data...\n",
    "    def __init__(self):\n",
    "        with open('class-0.txt','r') as class0:\n",
    "            class_0=[line.strip().split(' ') for line in class0]\n",
    "        with open('class-1.txt','r') as class1:\n",
    "            class_1=[line.strip().split(' ') for line in class1]\n",
    "        self.class0=class_0\n",
    "        self.class1=class_1\n",
    "    \n",
    "    @countcalls\n",
    "    def train_svm(parameters, x_train, y_train):\n",
    "        ## Populate the parameters...\n",
    "        gamma=parameters['gamma']\n",
    "        C=parameters['C']\n",
    "        kernel=parameters['kernel']\n",
    "        degree=parameters['degree']\n",
    "        coef0=parameters['coef0']\n",
    "        \n",
    "        ## Train the classifier...\n",
    "        clf = svm.SVC(kernel=kernel, C=C, gamma=gamma, degree=degree, coef0=coef0)\n",
    "        assert x_train.shape[0] <=541 and x_train.shape[1] <= 5720\n",
    "        clf.fit(x_train, y_train)\n",
    "        return clf\n",
    "    \n",
    "    ## Function to check the Modification Limits...(You can modify EXACTLY 20-DISTINCT TOKENS)\n",
    "    def check_data(self, original_file, modified_file):\n",
    "        with open(original_file, 'r') as infile:\n",
    "            data=[line.strip().split(' ') for line in infile]\n",
    "        Original={}\n",
    "        for idx in range(len(data)):\n",
    "            Original[idx] = data[idx]\n",
    "\n",
    "        with open(modified_file, 'r') as infile:\n",
    "            data=[line.strip().split(' ') for line in infile]\n",
    "        Modified={}\n",
    "        for idx in range(len(data)):\n",
    "            Modified[idx] = data[idx]\n",
    "\n",
    "        for k in sorted(Original.keys()):\n",
    "            record=set(Original[k])\n",
    "            sample=set(Modified[k])\n",
    "            assert len((set(record)-set(sample)) | (set(sample)-set(record)))==20\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "constants = ['#' * i for i in range(1,100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dt = None\n",
    "with open('test_data.txt', 'r') as infile:\n",
    "    test_dt = [line.strip().split(' ') for line in infile]\n",
    "' '.join(test_dt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "st = strategy()\n",
    "parameters = {\n",
    "    'C': 1,\n",
    "    'gamma': 'auto',\n",
    "    'kernel': 'linear',\n",
    "    'coef0': 0.0,\n",
    "    'degree': 3\n",
    "}\n",
    "\n",
    "lines = [' '.join(line) for line in st.class0] + [' '.join(line) for line in st.class1]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "# cv = TfidfVectorizer()\n",
    "cv = CountVectorizer()\n",
    "# print(X_train)\n",
    "X_train_tfidf = cv.fit_transform(lines)\n",
    "target = np.array([0] * 360 + [1] * 180)\n",
    "\n",
    "model = svm.SVC(kernel=parameters['kernel'], C=parameters['C'], gamma=parameters['gamma'], \n",
    "                degree=parameters['degree'], coef0=parameters['coef0'])\n",
    "model.fit(X_train_tfidf, target)\n",
    "\n",
    "test_lines = [' '.join(i) for i in test_dt]\n",
    "# test_dataset = cv.transform(test_lines)\n",
    "test_dataset = cv.transform(test_lines)\n",
    "predicted = model.predict(test_dataset)\n",
    "print('predicted1', np.mean(predicted == 1))\n",
    "\n",
    "top_coef_sorted = np.argsort(model.coef_.toarray()[0])[::-1]\n",
    "top_features = np.array(cv.get_feature_names())\n",
    "\n",
    "modified_list = []\n",
    "\n",
    "for record in test_dt:\n",
    "    record_new = record\n",
    "    for coef_index in top_coef_sorted:\n",
    "        feature = top_features[coef_index]\n",
    "\n",
    "        record_new = [word for word in record_new if word != feature]\n",
    "\n",
    "        if len((set(record) - set(record_new)) | \\\n",
    "               (set(record_new) - set(record))) == 20: # no more modifications\n",
    "            break       \n",
    "\n",
    "    if len((set(record) - set(record_new)) | \\\n",
    "               (set(record_new) - set(record))) != 20: \n",
    "        for const in constants:\n",
    "            if const not in record_new:\n",
    "                record_new += [const]\n",
    "            if len((set(record) - set(record_new)) | \\\n",
    "               (set(record_new) - set(record))) == 20: \n",
    "                break\n",
    "\n",
    "modified_list.append(record_new)\n",
    "\n",
    "# new_file = open(\"modified_data.txt\", \"w\")\n",
    "\n",
    "# for i in modified_list:\n",
    "#     new_file.write(' '.join(i))\n",
    "#     new_file.write('\\n')\n",
    "# new_file.close()\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))\n",
    "\n",
    "\n",
    "# modified_dt = None\n",
    "# with open('modified_data.txt', 'r') as infile:\n",
    "#     modified_dt = [line.strip().split(' ') for line in infile]\n",
    "# modified_dtset = cv.transform([' '.join(i) for i in modified_dt])\n",
    "# predicted2 = model.predict(modified_dtset)\n",
    "# np.mean(predicted2 == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "\n",
    "def change_set(line):\n",
    "    return set(line.split(' '))\n",
    "\n",
    "a = 'michael owen'\n",
    "change_set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0 1\n",
      " 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 1 0 1 0\n",
      " 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 1 1 1 0 0 0\n",
      " 1 0 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 0 1 1 1 1 1\n",
      " 0 1 0 0 1 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 1 1 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0\n",
      " 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "st = strategy()\n",
    "parameters = {\n",
    "    'C': 1,\n",
    "    'gamma': 'auto',\n",
    "    'kernel': 'linear',\n",
    "    'coef0': 0.0,\n",
    "    'degree': 3\n",
    "}\n",
    "lines = [' '.join(line) for line in st.class0] \\\n",
    "            + [' '.join(line) for line in st.class1]\n",
    "    \n",
    "mod_lines = None\n",
    "with open('modified_data.txt', 'r') as infile:\n",
    "    mod_lines = [line.strip() for line in infile]\n",
    "    \n",
    "test_lines = None\n",
    "with open('test_data.txt', 'r') as infile:\n",
    "    test_lines = [line.strip() for line in infile]\n",
    "\n",
    "target = np.array([0] * 360 + [1] * 180)\n",
    "cv = CountVectorizer()\n",
    "cv.fit(lines)\n",
    "X_train = cv.transform(lines)\n",
    "model = st.train_svm(parameters, X_train, np.array([0] * 360 + [1] * 180))\n",
    "features = np.array(cv.get_feature_names())\n",
    "\n",
    "# line0_sorted = np.argsort(X_train[0].toarray()[0])[::-1]\n",
    "# print(featues[line0_sorted[:96]])\n",
    "# print(X_train[0].toarray()[0][line0_sorted[:96]])\n",
    "\n",
    "print(model.predict(cv.transform(test_lines)))\n",
    "print(model.predict(cv.transform(mod_lines)))\n",
    "\n",
    "# np.where(features == 'hello')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = test_lines[0]\n",
    "l2 = mod_lines[0]\n",
    "\n",
    "diff1 = change_set(l1) - change_set(l2)\n",
    "diff2 = change_set(l2) - change_set(l1)\n",
    "removed = diff1 | diff2\n",
    "# print(removed, len(removed))\n",
    "# print(model.predict(cv.transform(test_lines[:1])))\n",
    "# print(model.predict(cv.transform(mod_lines[:1])))\n",
    "\n",
    "# temp = l1.split(\" \")\n",
    "# for word in list(removed):\n",
    "#     x, = np.where(featues == word)\n",
    "#     print(f'{word}\\t{model.coef_.data[x[0]]}')\n",
    "#     temp = [i for i in temp if i != word]\n",
    "#     print(model.predict(cv.transform([' '.join(temp)])))\n",
    "\n",
    "# coe_arr = []\n",
    "# for word in l1.split(\" \"):\n",
    "#     x, = np.where(features == word)\n",
    "#     if len(x) > 0:\n",
    "# #         print(f'{word}\\t{model.coef_.toarray()[0][x[0]]}')\n",
    "#         coe_arr.append((word, model.coef_.toarray()[0][x[0]]))\n",
    "\n",
    "# top_20 = sorted(coe_arr, key=lambda x: x[1], reverse=True)[:20]\n",
    "# temp = l1.split(\" \")\n",
    "# for word, _ in top_20:\n",
    "#     x, = np.where(featues == word)\n",
    "#     print(f'{word}\\t{model.coef_.data[x[0]]}')\n",
    "#     temp = [i for i in temp if i != word]\n",
    "#     print(model.predict(cv.transform([' '.join(temp)])))\n",
    "\n",
    "\n",
    "np.argsort(model.coef_.toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [-0.033706742840289503, -0.025667649604084575, -0.048741199668066004, -0.060958130817115583, 0.037578565133285953, 0.03045565583049753, 0.027046251179203847, 0.046543105543863844, -0.014483809105959157, -0.069304461977054865, 0.044895435233572173, -0.013897922431222242, -0.029180422206721485, 0.16730445540091685, -0.025375080556183528, -0.058229462455121353, 0.024256194988257512, -0.055916084763357049, 0.058073041841478844, -0.011532623928430056, -0.085578750026615344, 0.0038493433344678994, 0.046351509359357349, 0.03045565583049753, 0.052326105801658709, -0.12071944768867805, -0.10445832116566628, 0.063598764665613011, 0.1336985316446371, -0.0058358670369515406, -0.014800964793316218, -0.046132520041169828, -0.014800964793316218, -0.072807842031844094, -0.029539430348497257, -0.016529789157141955, 0.086581460912192432, -0.012059750636169715, -0.048741199668066004, -0.059698820187955595, 0.03045565583049753, 0.045594433550108109, 0.0062321636431825773, 0.06841788413420935, 0.054519806692008885, 0.079276622410511879, 0.046351509359357349, -0.014800964793316218, 0.065732551099928288, 0.012201600663164225, 0.082094578213351005, -0.15341143957055678, -0.016529789157141955, 0.054486108417417078, -0.014800964793316218, -0.079034888264888706, 0.07122728309724341, -0.0015632816322620896, -0.045086556725529213, 0.020306695753977116, -0.014800964793316218, -0.073693645185750972, -0.12057433233880716, 0.022880394970827195, 0.0027044140313002736, -0.024503129624850885, -0.014800964793316218, 0.14491958631214708, 0.10405286465262994, 0.13702530254640488, 0.0061051045063516343, 0.047146024505769442, -0.0083612863355028016, 0.0098955076628064673, -0.011643233593216529, -0.0023890325660017482, -0.010864281778394429, 0.052030638866491043, 0.03045565583049753, 0.060341955315721965, 0.030144900320601055, 0.035638238342581299, 0.036782081615101658, -0.016529789157141955, -0.071512439951866791, 0.081979427535624122, 0.097543605029522584, -0.0015632816322620896, -0.033706742840289503, 0.063176986315459163, 0.0098955076628064673, 0.010316164543263867, -0.051924640219383096, 0.03045565583049753, -0.014800964793316218, -0.014675483457957925]\n",
    "\n",
    "sorted(a)[::-1][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = X_train_tfidf\n",
    "y = np.array([0] * 360 + [1] * 180)\n",
    "clf_start = st.train_svm(parameters, X, y)\n",
    "param_range = np.arange(0.001,1,0.01) \n",
    "param_grid = [{'C': param_range, 'kernel': ['linear']}]\n",
    "grid = GridSearchCV(clf_start, param_grid)\n",
    "grid.fit(X,y)\n",
    "clf = grid.best_estimator_\n",
    "print(clf)\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t = model.coef_.data[top_coef_sorted]\n",
    "plt.plot(t)\n",
    "plt.show()\n",
    "\n",
    "model.coef_.data[top_coef_sorted][100], model.coef_.data[top_coef_sorted][-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_dt = None\n",
    "with open('modified_data.txt', 'r') as infile:\n",
    "    modified_dt = [line.strip().split(' ') for line in infile]\n",
    "modified_dtset = cv.transform([' '.join(i) for i in modified_dt])\n",
    "predicted2 = model.predict(modified_dtset)\n",
    "print(np.mean(predicted2 == 1))\n",
    "print(predicted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cv = sklearn.feature_extraction.text.CountVectorizer(vocabulary=top_features[top_position_coef])\n",
    "# count_array = cv.transform([' '.join(i) for i in test_dt]).toarray()\n",
    "# count_array.sum(axis=1)\n",
    "\n",
    "# print(np.sort(model.coef_.data))\n",
    "\n",
    "# print(model.coef_.data[np.argsort(model.coef_.data)[::-1][:10]])\n",
    "# print(model.coef_.data[np.argsort(model.coef_.data)[:10]])\n",
    "\n",
    "# strategy().check_data('./test_data.txt','./modified_data.txt')\n",
    "original_file = './test_data.txt'\n",
    "modified_file = './modified_data.txt'\n",
    "\n",
    "with open(original_file, 'r') as infile:\n",
    "    data=[line.strip().split(' ') for line in infile]\n",
    "Original={}\n",
    "for idx in range(len(data)):\n",
    "    Original[idx] = data[idx]\n",
    "\n",
    "with open(modified_file, 'r') as infile:\n",
    "    data=[line.strip().split(' ') for line in infile]\n",
    "Modified={}\n",
    "for idx in range(len(data)):\n",
    "    Modified[idx] = data[idx]\n",
    "    \n",
    "for k in sorted(Original.keys()):\n",
    "    record=set(Original[k])\n",
    "    sample=set(Modified[k])\n",
    "#     print(len(record), len(sample))\n",
    "#     print(set(record)-set(sample))\n",
    "#     print(set(sample)-set(record))\n",
    "    if len((set(record)-set(sample)) | (set(sample)-set(record))) != 20:\n",
    "        print(k, len(set(record)-set(sample)), len(set(sample)-set(record)))\n",
    "        print(set(record)-set(sample))\n",
    "        print(set(sample)-set(record))\n",
    "\n",
    "# len(Original[1]), len(Modified[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(cv.vocabulary_))\n",
    "# print(cv.get_feature_names()[-30:])\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "my_test = ['I am psa. michael', 'got damn it michael michael']\n",
    "cv2 = CountVectorizer()\n",
    "cv2.fit(my_test)\n",
    "# cv2.vocabulary_\n",
    "print(cv2.get_feature_names())\n",
    "X_train2 = cv2.transform(my_test)\n",
    "print(X_train2.toarray())\n",
    "model2 = svm.SVC(kernel='linear')\n",
    "# X_train2.shape, np.array([0, 0]).shape\n",
    "model2.fit(X_train2, np.array([0, 1]))\n",
    "# print(cv2.get_feature_names())\n",
    "print(model2.coef_.data)\n",
    "# print(model2.coef_.data, model2.coef_.shape)\n",
    "\n",
    "# print(np.argsort(model2.coef_.data)[::-1])\n",
    "# for i in cv2.transform(['I got got you','i miss you']).toarray():\n",
    "#     print(i)\n",
    "\n",
    "sorted_coef = np.argsort(model2.coef_.data)[::-1]\n",
    "print(model2.coef_.data[sorted_coef])\n",
    "print(np.array(cv2.get_feature_names())[sorted_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def fool_classifier(test_data): ## Please do not change the function defination...\n",
    "    ## Read the test data file, i.e., 'test_data.txt' from Present Working Directory...\n",
    "    test_dt = None\n",
    "    with open('test_data.txt', 'r') as infile:\n",
    "        test_dt = [line.strip().split(' ') for line in infile]\n",
    "    \n",
    "    ## You are supposed to use pre-defined class: 'strategy()' in the file `helper.py` for model training (if any),\n",
    "    #  and modifications limit checking\n",
    "#     constants = ['#' * i for i in range(100)]\n",
    "    strategy_instance = helper.strategy() \n",
    "    parameters = {\n",
    "        'C': 1,\n",
    "        'gamma': 'auto',\n",
    "        'kernel': 'linear',\n",
    "        'coef0': 0.0,\n",
    "        'degree': 3\n",
    "    }\n",
    "    lines = [' '.join(line) for line in strategy_instance.class0] \\\n",
    "            + [' '.join(line) for line in strategy_instance.class1]\n",
    "    \n",
    "    cv = CountVectorizer()\n",
    "    cv.fit(lines)\n",
    "    X_train = cv.transform(lines)\n",
    "    model = strategy_instance.train_svm(parameters, X_train, np.array([0] * 360 + [1] * 180))\n",
    "    top_positive_coef = np.argsort(model.coef_.data)[::-1]\n",
    "    top_features = np.array(cv.get_feature_names())\n",
    "    ##..................................#\n",
    "    modified_list = []\n",
    "\n",
    "    for record in test_dt:\n",
    "        record_new = record\n",
    "        for coef_index in top_coef_sorted:\n",
    "            feature = top_features[coef_index]\n",
    "            feature_coef = model.coef_.data[coef_index]\n",
    "            record_new = [word for word in record_new if word != feature]\n",
    "\n",
    "            if len((set(record) - set(record_new)) | \\\n",
    "                   (set(record_new) - set(record))) == 20: # no more modifications\n",
    "                break\n",
    "\n",
    "        modified_list.append(record_new)\n",
    "    \n",
    "    ## Write out the modified file, i.e., 'modified_data.txt' in Present Working Directory...\n",
    "    new_file = open(\"modified_data.txt\", \"w\")\n",
    "\n",
    "    for i in modified_list:\n",
    "        new_file.write(' '.join(i))\n",
    "        new_file.write('\\n')\n",
    "    new_file.close()\n",
    "    \n",
    "    ## You can check that the modified text is within the modification limits.\n",
    "    modified_data='./modified_data.txt'\n",
    "    assert strategy_instance.check_data(test_data, modified_data)\n",
    "    return strategy_instance ## NOTE: You are required to return the instance of this class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **NOTE:** \n",
    " 1. **You are required to return the instance of the class: `strategy()`, $\\textit{e.g.}$, `strategy_instance` in the above cell.**\n",
    " 2. **You are supposed to write out the file `modified_data.txt` in the same directory, and in the same format as that of `test_data.txt`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "import submission as submission\n",
    "test_data='./test_data.txt'\n",
    "strategy_instance = submission.fool_classifier(test_data)\n",
    "\n",
    "########\n",
    "#\n",
    "# Testing Script.......\n",
    "#\n",
    "#\n",
    "########\n",
    "\n",
    "print('Success %-age = {}-%'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import helper\n",
    "import submission as submission\n",
    "test_data='./test_data.txt'\n",
    "strategy_instance = submission.fool_classifier(test_data)\n",
    "print('hello')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATION:\n",
    "\n",
    "1. For evaluation, we will consider a bunch of test paragraphs having:\n",
    "    * Approximately 500-1500 test samples for class-1, with each line corresponding to a distinct test sample.The input test file will follow the same format as that of `test_data.txt`.\n",
    "    * We will consider the success rate of your algorithm for final evaluation. By success rate we mean %-age of samples miss-classified by the `target-classifier` ($\\textit{i.e.,}$  instances of `class-1`, classified as `class-0` after `20` distinct modifications). \n",
    "\n",
    "### Example:\n",
    "\n",
    "1. Consider 200 test-samples (classified as **class-1** by the `target-classifier`). \n",
    "2. For-Example, after modifying each test sample by (**20 DISTINCT TOKENS**) the `target-classifier` mis-classifies **100** test samples ($\\textit{i.e.,}$ 100 test samples are classified as **class-0** then your **success %-age** is:\n",
    "\n",
    "3. **success %-age** = (100) x 100/200 = **50%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make own model\n",
    "# check it against test data\n",
    "# find out top coefficient (positive), sort it by value\n",
    "# modify test data according to the top coefficient\n",
    "# when I put in the parameter, the kernel should be linear\n",
    "# if feature is positive and it's in the record remove that\n",
    "            # if negatvie and it;s in thre record, add 20 distinct words\n",
    "            #if negative and it's not in the record, add it\n",
    "\n",
    "'''\n",
    ">>> x = np.arange(9.).reshape(3, 3)\n",
    ">>> np.where( x > 5 )\n",
    "(array([2, 2, 2]), array([0, 1, 2]))\n",
    ">>> x[np.where( x > 3.0 )]               # Note: result is 1D.\n",
    "array([ 4.,  5.,  6.,  7.,  8.])\n",
    ">>> np.where(x < 5, x, -1)               # Note: broadcasting.\n",
    "array([[ 0.,  1.,  2.],\n",
    "       [ 3.,  4., -1.],\n",
    "       [-1., -1., -1.]])\n",
    "'''\n",
    "\n",
    "x = np.arange(9.).reshape(3, 3)\n",
    "print(x)\n",
    "# print(np.where( x > 5 ))\n",
    "# print(x[np.where( x > 3.0 )] )\n",
    "print(np.where(x < 5, x, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = ['a', 'b', 'c', 'c', 'd', 'e']\n",
    "# [i for i in a if i != 'c']\n",
    "\n",
    "np.array(a)\n",
    "\n",
    "test_file = open(\"test_write.txt\", \"w\")\n",
    "\n",
    "# file.write(“This is a test”) \n",
    "# file.write(“To add more lines.”)\n",
    "for i in a:\n",
    "    test_file.write(i + \"\\n\")\n",
    "\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4, 4]\n",
    "# len(set(a))\n",
    "\n",
    "# ['hello', 'world'] + ['michael']\n",
    "\n",
    "# for i in ab:\n",
    "#     cd = i.split(\" \")\n",
    "#     print(cd[1], cd[2], int(cd[1]) | int(cd[2]))\n",
    "\n",
    "# np.where(np.array(a) > 3)[0]\n",
    "\n",
    "ab = [('mi', 2), ('sad', 5), ('owen', 1), ]\n",
    "sorted(ab, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for record in test_dt[:1]:\n",
    "    del_count = 0\n",
    "    record_new = record\n",
    "    for feature in top_features[top_position_coef][:20]:\n",
    "        feat_count = record_new.count(feature)\n",
    "        if del_count + feat_count <= 20:\n",
    "            del_count += feat_count\n",
    "            record_new = [word for word in record_new if word != feature]\n",
    "        else:\n",
    "            while del_count != 20:\n",
    "                if feature in record_new:\n",
    "                    record_new.remove(feature)\n",
    "                    del_count += 1\n",
    "                else:\n",
    "                    break\n",
    "            if del_count == 20:\n",
    "                break\n",
    "    modified_list.append(record_new)\n",
    "    print(f'original count: {len(record)}, modified count: {len(record_new)}')\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
