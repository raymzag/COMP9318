{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP9318 Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "1. This note book contains instructions for **COMP9318-lab4**.\n",
    "\n",
    "* You are required to complete your implementation in a file `submission.py` provided along with this notebook.\n",
    "\n",
    "* You are not allowed to print out unnecessary stuff. We will not consider any output printed out on the screen. All results should be returned in appropriate data structures via corresponding functions.\n",
    "\n",
    "* You can submit your implementation for **lab4** via following link: http://kg.cse.unsw.edu.au:8318/lab4/ .\n",
    "\n",
    "* For each question, we have provided you with detailed instructions along with question headings. In case of any problem, you can post your query @ Piazza.\n",
    "\n",
    "* You are allowed to add other functions and/or import modules (you may have to in this lab), but you are not allowed to define global variables. **Only functions are allowed** in `submission.py`. \n",
    "\n",
    "* You should not import unnecessary modules/libraries, failing to import such modules at test time will lead to errors.\n",
    "\n",
    "* We will provide immediate feedback on your submission. You can access your scores using the online submission portal on the same day. \n",
    "\n",
    "* For **Final Evaluation** we will be using a different dataset, so your final scores may vary.  \n",
    "\n",
    "* You are allowed to submit as many times as you want before the deadline, but **ONLY the latest version will be kept and marked**.\n",
    "\n",
    "* Submission deadline for this assignment is **23:59:59 on 22nd May, 2018**. We will **not** accept any late submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question-1: Logistic Regression using Gradient Descent\n",
    "\n",
    "* In this lab you are required to implement Logistic Regression using Gradient Descent.\n",
    "* The training data is 2-dimensional data points from two classes namely: class-0, and class-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Col1</th>\n",
       "      <th>Col2</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.464726</td>\n",
       "      <td>-0.552165</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.465277</td>\n",
       "      <td>0.155947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.367907</td>\n",
       "      <td>0.337509</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.703355</td>\n",
       "      <td>-0.511965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.193367</td>\n",
       "      <td>0.642282</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Col1      Col2  Label\n",
       "0  0.464726 -0.552165    0.0\n",
       "1 -0.465277  0.155947    0.0\n",
       "2 -0.367907  0.337509    0.0\n",
       "3 -1.703355 -0.511965    0.0\n",
       "4 -0.193367  0.642282    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_file='./asset/a'\n",
    "raw_data = pd.read_csv(data_file, sep=',')\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to **implement** logistic regression classifier (i.e., `logistic_regression()` in the file: `submission.py`). The input arguments of `logistic_regression()` are:\n",
    "* `data`: Data in 2-columns format\n",
    "* `labels`: Class-labels\n",
    "* `coefficients`: Logistic Regression coefficients to be computed, initialized with $ZEROS$\n",
    "* `num_epochs`: Number of epochs\n",
    "* `learning_rate`: Learning rate of the algorithm \n",
    "\n",
    "The return value of `logistic_regression()` should be a numpy array containing the logistic regression **coefficients**. The coefficient corresponding to intercept term should be appended at the start of the array.\n",
    "\n",
    "For example, a sample output is shown in the cell given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6000, 2), dtype('float64'), 12000, 8, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read in the Data...\n",
    "data_file='./asset/a'\n",
    "raw_data = pd.read_csv(data_file, sep=',')\n",
    "labels=raw_data['Label'].values\n",
    "data=np.stack((raw_data['Col1'].values,raw_data['Col2'].values), axis=-1)\n",
    "\n",
    "## Fixed Parameters. Please do not change values of these parameters...\n",
    "coefficients = np.zeros(3) # We initialize the coefficients with ZERO. We also compute the intercept term.\n",
    "num_epochs = 20000\n",
    "learning_rate = 50e-5\n",
    "\n",
    "# data, labels, coefficients, num_epochs, learning_rate\n",
    "data.shape, data.dtype, data.size, data.itemsize, data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005 20000\n",
      "\n",
      " Logisitc Regression bias(intercept) term : -10.2034290687\n",
      "\n",
      " Logisitc Regression estimated coefficients : [-2.29067514  5.31366198]\n"
     ]
    }
   ],
   "source": [
    "X = data\n",
    "y = labels\n",
    "\n",
    "# features standerdization\n",
    "# X_std = np.copy(X)\n",
    "\n",
    "# X_std[:,0] = (X_std[:,0] - X_std[:,0].mean()) / X_std[:,0].std()\n",
    "# X_std[:,1] = (X_std[:,1] - X_std[:,1].mean()) / X_std[:,1].std()\n",
    "\n",
    "# Define Logistic Regression hypothesis or sigmoid function\n",
    "\n",
    "def sigmoid(X, theta):\n",
    "    \n",
    "    z = np.dot(X, theta[1:]) + theta[0]\n",
    "    \n",
    "    return 1.0 / ( 1.0 + np.exp(-z))\n",
    "\n",
    "# Define Logistic Regression Cost Function\n",
    "def lrCostFunction(y, hx):\n",
    "  \n",
    "    # compute cost for given theta parameters\n",
    "    j = -y.dot(np.log(hx)) - ((1 - y).dot(np.log(1-hx)))\n",
    "    \n",
    "    return j\n",
    "\n",
    "# Gradient Descent function to minimize the Logistic Regression Cost Function.\n",
    "def lrGradient(X, y, theta, alpha, num_iter):\n",
    "    # empty list to store the value of the cost function over number of iterations\n",
    "    cost = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        # call sigmoid function \n",
    "        hx = sigmoid(X, theta)\n",
    "        # calculate error\n",
    "        error = hx - y\n",
    "        # calculate gradient\n",
    "        grad = X.T.dot(error)\n",
    "        # update values in theta\n",
    "        theta[0] = theta[0] - alpha * error.sum()\n",
    "        theta[1:] = theta[1:] - alpha * grad\n",
    "        \n",
    "        cost.append(lrCostFunction(y, hx))\n",
    "        \n",
    "    return cost   \n",
    "\n",
    "# m = Number of training examples\n",
    "# n = number of features\n",
    "m, n = X.shape\n",
    "\n",
    "# set learning rate to 0.01 and number of iterations to 500\n",
    "alpha = learning_rate\n",
    "num_iter = 20000\n",
    "theta = coefficients\n",
    "\n",
    "print(alpha, num_iter)\n",
    "\n",
    "# cost = lrGradient(X_std, y, theta, alpha, num_iter)\n",
    "cost = lrGradient(X, y, theta, alpha, num_iter)\n",
    "\n",
    "# print theta paramters \n",
    "print ('\\n Logisitc Regression bias(intercept) term :', theta[0])\n",
    "print ('\\n Logisitc Regression estimated coefficients :', theta[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10.20342907,  -2.29067514,   5.31366198])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(data, coefficients): \n",
    "    z = np.dot(data, coefficients[1:]) + coefficients[0]\n",
    "    result = 1.0 / (1.0 + np.exp(-z))  \n",
    "    return result\n",
    "\n",
    "def logistic_regression(data, labels, coefficients, num_epochs, learning_rate):\n",
    "    for i in range(num_epochs):\n",
    "        hx = sigmoid(data, coefficients)\n",
    "        error = hx - labels\n",
    "        gradient = data.T.dot(error)\n",
    "        coefficients[0] -= learning_rate * error.sum()\n",
    "        coefficients[1:] -= learning_rate * gradient\n",
    "        \n",
    "    return coefficients\n",
    "\n",
    "logistic_regression(data, labels, coefficients, num_epochs, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.20342907  -2.29067514   5.31366198]\n"
     ]
    }
   ],
   "source": [
    "import submission as submission\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## Read in the Data...\n",
    "data_file='./asset/a'\n",
    "raw_data = pd.read_csv(data_file, sep=',')\n",
    "labels=raw_data['Label'].values\n",
    "data=np.stack((raw_data['Col1'].values,raw_data['Col2'].values), axis=-1)\n",
    "\n",
    "## Fixed Parameters. Please do not change values of these parameters...\n",
    "coefficients = np.zeros(3) # We initialize the coefficients with ZERO. We also compute the intercept term.\n",
    "num_epochs = 20000\n",
    "learning_rate = 50e-5\n",
    "\n",
    "coefficients=submission.logistic_regression(data, labels, coefficients, num_epochs, learning_rate)\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-10.20342907  -2.29067514   5.31366198]\n"
     ]
    }
   ],
   "source": [
    "import submission as submission\n",
    "\n",
    "## Read in the Data...\n",
    "data_file='./asset/a'\n",
    "raw_data = pd.read_csv(data_file, sep=',')\n",
    "labels=raw_data['Label'].values\n",
    "data=np.stack((raw_data['Col1'].values,raw_data['Col2'].values), axis=-1)\n",
    "\n",
    "## Fixed Parameters. Please do not change values of these parameters...\n",
    "coefficients = np.zeros(3) # We initialize the coefficients with ZERO. We also compute the intercept term.\n",
    "num_epochs = 20000\n",
    "learning_rate = 50e-5\n",
    "\n",
    "coefficients=submission.logistic_regression(data, labels, coefficients, num_epochs, learning_rate)\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You need to complete the function `logistic_regression()` in the file: `submission.py`. You can test your submission against sample test cases via online submission system (i.e., http://kg.cse.unsw.edu.au:8318/lab4/).\n",
    "\n",
    "\n",
    "# Test Environment\n",
    "\n",
    "For testing, we have pre-installed the requisite modules and/or libraries in the testing environment. You are only allowed to use following libraries:\n",
    "* python: 3.5.2\n",
    "* pandas: 0.19.2\n",
    "* numpy: 1.14.0\n",
    "\n",
    "NOTE: <br> \n",
    "* You are required to implement the gradint descent for logistic regression by yourself. You are not allowed to import **sklearn** and/or any other library in Lab4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
